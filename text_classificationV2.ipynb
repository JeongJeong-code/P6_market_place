{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classificationV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxEwnNc62zsWSR4nciGm8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeongJeong-code/P6_market_place/blob/main/text_classificationV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swm-HL5HhbeE",
        "outputId": "e8a4db22-b082-494c-854e-c754cc990e74"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from enum import Enum\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os.path\n",
        "import missingno as mi\n",
        "import re\n",
        "import sklearn as sk\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics.cluster import homogeneity_score\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.cluster import (AgglomerativeClustering,\n",
        "                             KMeans, DBSCAN, SpectralClustering)\n",
        "from sklearn.metrics import davies_bouldin_score, silhouette_score\n",
        "! {sys.executable} -m pip install ipynb\n",
        "\n",
        "import cv2\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipynb in /usr/local/lib/python3.7/dist-packages (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8aLtI58hdrl",
        "outputId": "7af76661-f392-4de9-89e4-79ae3f4f5080"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S--MLIzxhfTs"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/JeongJeong-code/P6_market_place/main/df1_test.csv'\n",
        "df1_test= pd.read_csv(url)\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd7HOkJkhhal"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk.stem\n",
        "import string"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gm9yRAUGiTM2"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokenize_desc = pd.Series(df1_test.description.str.lower().apply(tokenizer.tokenize),name ='tokenize_desc')"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4VsnOXWo0Iz"
      },
      "source": [
        ""
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMdR9zDOicJR"
      },
      "source": [
        "all_words = [word for tokens in tokenize_desc for word in tokens]\n",
        "from collections import Counter\n",
        "count_all_words = Counter(all_words)\n",
        "#count_all_words.most_common(100)"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh9xEv1Ciwh1"
      },
      "source": [
        "stopword_list = nltk_stop_words = nltk.corpus.stopwords.words('english') + list(count_all_words.most_common(100))\n"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeWFg7J8jScq"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "y = pd.Series(le.fit_transform(df1_test.categories),name='class_labels')\n",
        "\n",
        "\n"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4rdo5P7oq5k"
      },
      "source": [
        ""
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8FFLp9Wjkiu"
      },
      "source": [
        "list_labels = y\n",
        "list_tokens = tokenize_desc"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6PDb0ZzkKSU",
        "outputId": "8194b860-260c-4390-f157-5b22624673b4"
      },
      "source": [
        "list_tokens"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      [key, features, of, elegance, polyester, multi...\n",
              "1      [specifications, of, sathiyas, cotton, bath, t...\n",
              "2      [key, features, of, eurospa, cotton, terry, fa...\n",
              "3      [key, features, of, santosh, royal, fashion, c...\n",
              "4      [key, features, of, jaipur, print, cotton, flo...\n",
              "                             ...                        \n",
              "923    [oren, empower, extra, large, self, adhesive, ...\n",
              "924    [wallmantra, large, vinyl, sticker, sticker, p...\n",
              "925    [buy, uberlyfe, extra, large, pigmented, polyv...\n",
              "926    [buy, wallmantra, medium, vinyl, sticker, stic...\n",
              "927    [buy, uberlyfe, large, vinyl, sticker, for, rs...\n",
              "Name: tokenize_desc, Length: 928, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPJbC2-skiCg"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(df1_test.description,df.class_labels, test_size=0.1, random_state=40)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeIaKG1SnCER",
        "outputId": "092f03fc-0853-4b8b-918e-3fed0ac3efd7"
      },
      "source": [
        "X_train[:10]"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "787    Woven Terry Cotton Bath Towel (Bath Towel, Mul...\n",
              "104    Specifications of Toffyhouse Baby Boy's Sleeps...\n",
              "600    Buy Welhouse Cotton Hand & Face Towel Set at R...\n",
              "454    Sonata 77036SM02J Watch - Buy Sonata 77036SM02...\n",
              "31     Lenco Bdblue Tango Analog Watch  - For Men, Bo...\n",
              "262    Specifications of Yves Bertelin YBSCR557 Analo...\n",
              "253    Ndura Kadhai 1.7 L\\r\\n                        ...\n",
              "92     Artisan Creation Checkered Single Quilts & Com...\n",
              "370    Buy Zyxel VMG1312-B10A VDSL2 Wireless N VDSL2 ...\n",
              "204    Agromech YIBOO Rolling Pizza Cutter (Stainless...\n",
              "Name: description, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEN-RdQClKIS",
        "outputId": "eaa1a2bf-af6d-4500-bf41-d2d8f51cd16d"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer(analyzer='word', token_pattern=r'\\w+',stop_words=stopword_list)\n",
        "\n",
        "bow = dict()\n",
        "bow[\"train\"] = (count_vectorizer.fit_transform(X_train), y_train)\n",
        "bow[\"test\"]  = (count_vectorizer.transform(X_test), y_test)\n",
        "print(bow[\"train\"][0].shape)\n",
        "print(bow[\"test\"][0].shape)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(835, 4382)\n",
            "(93, 4382)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNPhzLlJlm-Z"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w+',stop_words=stopword_list)\n",
        "\n",
        "tfidf = dict()\n",
        "tfidf[\"train\"] = (tfidf_vectorizer.fit_transform(X_train), y_train)\n",
        "tfidf[\"test\"]  = (tfidf_vectorizer.transform(X_test), y_test)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ctmojs2irIaD"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb_classifier = MultinomialNB()"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y8DovnvrhFy",
        "outputId": "e08cb75e-bdb9-4c18-da9c-df4818f257b0"
      },
      "source": [
        "nb_classifier.fit(*bow[\"train\"])"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf_tL5NHrmJH",
        "outputId": "9c1fafed-3e4b-49d8-8e20-a99b32dae9e0"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "cross_val_score(nb_classifier, *bow[\"train\"], cv=5, scoring='accuracy')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.75449102, 0.77245509, 0.78443114, 0.79041916, 0.77245509])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4QoOOsgsQmy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}